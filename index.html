<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Understanding Slimness and Sparsity in Unified Multimodal Models: An Empirical Study</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://shwai-he.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Understanding Slimness and Sparsity in Unified Multimodal Models: An Empirical Study</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shwai-he.github.io/">Shwai He</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://v3alab.github.io/author/chaorui-deng/">Chaorui Deng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ang-li.com/">Ang Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://shenyann.github.io/">Shen Yan</a><sup>1,†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ByteDance Seed,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Shwai-He/SparseUnifiedModel"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====================== Abstract Section ====================== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale multimodal models have achieved remarkable progress in both understanding and generation. Traditionally, these tasks were studied in isolation, resulting in distinct architectures. 
            Recent efforts instead pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. 
            However, such unification introduces inference inefficiencies arising from <i>task-specific activation</i>, <i>compute imbalance</i>, and <i>input variability</i>.
            Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited.
            In this work, we conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. 
            Our study reveals that the understanding component, while essential for multimodal reasoning, exhibits notable compressibility in generation tasks. 
            In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate pruning ratios. 
            To address this limitation, we propose a Mixture-of-Experts (MoE) Adaptation, inspired by dynamic activation patterns observed across different samples. 
            This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. 
            We validate the effectiveness of sparse activation through expert-frozen tuning and further show that a fully trainable adaptation delivers additional gains. 
            As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====================== Overview Section ====================== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png">
          SparseUnifiedModel: Toward Efficient Unified Multimodal Architectures
        </h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>SparseUnifiedModel</b> integrates pre-trained multimodal backbones 
            (<a href="https://github.com/ByteDance-Seed/Bagel">BAGEL</a>, 
            <a href="https://github.com/inclusionAI/Ming/tree/main">Ming-Omni</a>, 
            <a href="https://github.com/QwenLM/Qwen-Image">Qwen-Image</a>)
            into a unified framework for both <i>understanding</i> and <i>generation</i>. 
            It leverages sparse activation, modular adaptation, and component-level analysis 
            to uncover redundancy and improve computational efficiency across heterogeneous modalities.
            We consider a two-stage efficiency optimization procedure:
          </p>
          <ul type="1">
            <li><b>Stage 1: Training-Free Component Analysis.</b> Using depth and width pruning as probing tools, we systematically identify compressible submodules, revealing distinct sensitivities across understanding and generation components.</li>
            <li><b>Stage 2: Sparse Adaptation via MoE Integration.</b> Inspired by dynamic activation patterns across tasks, we introduce a <b>Mixture-of-Experts (MoE)</b> adaptation that enables selective activation in generation modules, achieving high-quality generation with reduced computational cost.</li>
          </ul>
        </div>
        <div class="has-text-centered">
          <img id="teaser" width="80%" src="static/images/efficient_ug.svg">
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====================== Key Results and Insights ====================== -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Results and Insights</h2>
    <hr>

    <!-- Depth Reduction Analysis (3 Figures in Row) -->
    <div class="columns is-centered is-vcentered">
      <div class="column is-one-third">
        <figure>
          <img src="static/images/depth_reduction.svg" style="width:95%; height:auto;">
          <figcaption class="is-size-7 mt-2"><b>(a)</b> Overall pruning trend across model depth.</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure>
          <img src="static/images/compress_und_und_task.png" style="width:95%; height:auto;">
          <figcaption class="is-size-7 mt-2"><b>(b)</b> Understanding component under <i>understanding tasks</i>.</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure>
          <img src="static/images/compress_und_gen_task.png" style="width:95%; height:auto;">
          <figcaption class="is-size-7 mt-2"><b>(c)</b> Understanding component under <i>generation tasks</i>.</figcaption>
        </figure>
      </div>
    </div>

    <p class="is-size-6 mt-3">
      <b>Figure 2.</b> <b>Depth reduction analysis.</b> 
      The understanding component shows higher compressibility overall, 
      yet tolerates even larger pruning ratios under <i>generation tasks</i>, 
      suggesting cross-task redundancy in unified multimodal representations.
    </p>
  </div>
</section>

<!-- ====================== Neuron Overlap and Calibration ====================== -->
<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Neuron Overlap and Calibration</h3>
    <div class="content has-text-centered">

      <!-- (a) Overlap -->
      <figure>
        <img src="static/images/overlap.svg" width="70%">
        <figcaption class="is-size-7 mt-2"><b>(a)</b> Neuron activation overlap between understanding and generation tasks.</figcaption>
      </figure>
      <p class="is-size-6 mt-3">
        <b>Figure 3.</b> The overlap ratio quantifies shared neurons activated by both understanding and generation inputs, 
        highlighting <b>task-specific specialization</b> within the unified multimodal model.
      </p>

      <!-- (b) Calibration -->
      <figure class="mt-5">
        <img src="static/images/ablate_data.png" width="70%">
        <figcaption class="is-size-7 mt-2"><b>(b)</b> Effect of calibration data on neuron importance estimation and overlap ratio.</figcaption>
      </figure>
      <p class="is-size-6 mt-3">
        Using task-aligned calibration data significantly improves the reliability of neuron importance estimation, confirming that <b>calibration data matters</b>.
      </p>

    </div>
  </div>
</section>

<!-- ====================== Sparsity and Comparison ====================== -->




<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Emergent Sparsity and Model Comparison</h3>
    <div class="content has-text-centered">

      <figure>
        <img src="static/images/dilemma_compress_gen.png" width="70%">
        <figcaption class="is-size-7 mt-2">
          <b>Figure 3.</b> The compression dilemma in generation modules.
        </figcaption>
      </figure>

      <p class="is-size-6 mt-3">
        While <b>understanding components</b> exhibit high structural redundancy and can be pruned with minor impact,
        <b>generation modules</b> present a fundamental compression dilemma. 
        Their parameters are tightly coupled with output fidelity, where even moderate pruning ratios 
        lead to severe degradation in text or image quality.  
        This imbalance highlights a key challenge in unified multimodal efficiency — 
        <b>how to compress aggressively without harming generative capability</b>.  
        In our study, we find that sparse adaptation via MoE routing offers a viable path forward, 
        allowing selective activation instead of uniform compression across the entire generation stack.
      </p>

      <figure>
        <img src="static/images/zeros_ones_ratio_hatch.svg" width="70%">
        <figcaption class="is-size-7 mt-2"><b>(a)</b> Activation sparsity patterns across layers.</figcaption>
      </figure>

      <p class="is-size-6 mt-3">
        <b>Figure 5.</b> Visualization of activation distributions shows an increasing ratio of inactive neurons (<i>zeros</i>) in higher layers, 
        indicating emergent structured sparsity that can be exploited for efficient routing and inference.
      </p>

      <figure class="mt-5">
        <img src="static/images/comprison_gen.png" width="70%">
        <figcaption class="is-size-7 mt-2"><b>(b)</b> Overall comparison between training-free slimness and training-aware sparsity.</figcaption>
      </figure>

      <p class="is-size-6 mt-3">
        Training-free analysis identifies naturally compressible regions, 
        while MoE-based sparse adaptation further refines activation patterns to restore generation quality with minimal computation.
      </p>

    </div>
  </div>
</section>



</section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  </code></pre>
    </div>
  </section>

</body>
</html>
