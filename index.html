<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Understanding Slimness and Sparsity in Unified Multimodal Models: An Empirical Study</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://shwai-he.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Understanding Slimness and Sparsity in Unified Multimodal Models: An Empirical Study</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shwai-he.github.io/">Shwai He</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://v3alab.github.io/author/chaorui-deng/">Chaorui Deng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ang-li.com/">Ang Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://shenyann.github.io/">Shen Yan</a><sup>1,†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ByteDance Seed,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Shwai-He/SparseUnifiedModel"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale multimodal models have achieved remarkable progress in both understanding and generation. Traditionally, these tasks were studied in isolation, resulting in distinct architectures. 
            Recent efforts instead pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. 
            However, such unification introduces inference inefficiencies arising from \textit{task-specific activation}, \textit{compute imbalance}, and \textit{input variability}.
            Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited.
            In this work, we conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. 
            Our study reveals that the understanding component, while essential for multimodal reasoning, exhibits notable compressibility in generation tasks. 
            In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate pruning ratios. 
            To address this limitation, we propose a Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. 
            This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. 
            We first validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. 
            As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> SparseUnifiedModel: Toward Efficient Unified Multimodal Architectures
</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>

          <b>SparseUnifiedModel</b> integrates pre-trained multimodal backbones 
          (<a href="https://github.com/ByteDance-Seed/Bagel">BAGEL</a>, <a href="https://github.com/inclusionAI/Ming/tree/main">Ming-Omni</a>, <a href="https://github.com/QwenLM/Qwen-Image">Qwen-Image</a>)
          into a unified framework for both <i>understanding</i> and <i>generation</i>. 
          It leverages sparse activation, modular adaptation, and component-level analysis 
          to uncover redundancy and improve computational efficiency across heterogeneous modalities.
          We consider a two-stage efficiency optimization procedure:
          <ul type="1">
            <li><b>Stage 1: Training-Free Component Analysis</b>. 
              <span style="font-size: 95%;">
                Using depth and width pruning as probing tools, we systematically identify compressible submodules within unified multimodal models, 
                revealing the sensitivity difference between understanding and generation components.
              </span>
            </li>
            <li><b>Stage 2: Sparse Adaptation via MoE Integration</b>. 
              <span style="font-size: 95%;">
                Inspired by dynamic activation patterns across tasks, we introduce a 
                <b>Mixture-of-Experts (MoE)</b> adaptation to enable selective activation in generation modules, 
                achieving high-quality generation with reduced computational cost.
              </span>
            </li>
          </ul>
        </p>
      </div>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="80%" src="static/images/efficient_ug.svg">     
        </div>
      </centering>           
    </div>
  </div>
</div>



<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3 has-text-centered">Architecture and Analysis</h2>
    <hr>


    <!-- ---------------------- Depth Reduction ---------------------- -->
    <div class="content has-text-centered">
      <img src="static/images/depth_reduction.svg" width="65%">
      <p class="is-size-6 mt-3">
        <b>Figure 2.</b> <b>Depth reduction analysis.</b> 
        We progressively prune layers across understanding and generation components. 
        The results show that understanding layers are more compressible, while generation layers 
        exhibit sharp performance degradation under the same pruning ratio, 
        reflecting distinct depth sensitivities across modalities.
      </p>
    </div>

    <!-- ---------------------- Neuron Overlap ---------------------- -->
    <div class="content has-text-centered">
      <img src="static/images/overlap.svg" width="65%">
      <p class="is-size-6 mt-3">
        <b>Figure 3.</b> <b>Neuron activation overlap across tasks.</b> 
        Each point represents the proportion of neurons activated by both understanding and generation inputs. 
        The low overlap ratio reveals <b>task-specific specialization</b> and motivates 
        component-wise partitioning strategies for efficient adaptation.
      </p>
    </div>

    <!-- ---------------------- Runtime / Loss ---------------------- -->
    <div class="content has-text-centered">
      <img src="static/images/rt_loss.svg" width="65%">
      <p class="is-size-6 mt-3">
        <b>Figure 4.</b> <b>Efficiency–performance trade-off.</b> 
        We analyze runtime and reconstruction loss under varying compression ratios. 
        Sparse adaptation through MoE routing effectively recovers performance 
        while significantly reducing computational cost, achieving a balanced Pareto frontier.
      </p>
    </div>

    <!-- ---------------------- Sparsity Ratio ---------------------- -->
    <div class="content has-text-centered">
      <img src="static/images/zeros_ones_ratio_hatch.svg" width="65%">
      <p class="is-size-6 mt-3">
        <b>Figure 5.</b> <b>Emergent sparsity patterns.</b> 
        Visualization of activation distributions across layers. 
        The increasing ratio of inactive neurons (<i>zeros</i>) in higher layers indicates 
        an emergent sparse structure that can be exploited for efficient routing and inference.
      </p>
    </div>

  </div>
</section>


</section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  </code></pre>
    </div>
  </section>

</body>
</html>
